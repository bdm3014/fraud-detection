{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Proccessed Data from the Data Pre-Proccessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'train_12.pkl' pickle file and load it into the 'train' DataFrame\n",
    "train = pd.read_pickle('./train_12.pkl')\n",
    "\n",
    "# Read the 'test_12.pkl' pickle file and load it into the 'test' DataFrame\n",
    "test = pd.read_pickle('./test_12.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the 'Date' Columns from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date columns are not indicators\n",
    "train.drop('TransactionDT', axis=1, inplace=True)\n",
    "train.drop('DT', axis=1, inplace=True)\n",
    "\n",
    "test.drop('TransactionDT', axis=1, inplace=True)\n",
    "test.drop('DT', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable for training set (y_train)\n",
    "y_train = train['isFraud']\n",
    "\n",
    "# Independent variables for training set (X_train)\n",
    "X_train = train.drop(['isFraud'], axis=1)\n",
    "\n",
    "# Target variable for test set (y_test)\n",
    "y_test = test['isFraud']\n",
    "\n",
    "# Independent variables for test set (X_test)\n",
    "X_test = test.drop(['isFraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of negative and positive examples\n",
    "count_negative = (y_train == 0).sum()\n",
    "count_positive = (y_train == 1).sum()\n",
    "\n",
    "# Calculate the value of scale_pos_weight\n",
    "scale_pos_weight = math.sqrt(count_negative / count_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LightGBM model woth Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defualt Parameter\n",
    "lgbclf = lgb.LGBMClassifier(\n",
    "  random_state=1003,\n",
    "  scale_pos_weight=scale_pos_weight,\n",
    "  metric='auc',\n",
    "  objective= 'binary',\n",
    "  device = 'gpu')\n",
    "lgbclf.fit(X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_lgbm = lgbclf.predict(X_test)\n",
    "\n",
    "# Probas for train\n",
    "y_train_lgbm_proba = lgbclf.predict_proba(X_train)[:, 1]  \n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_train_lgbm_proba)\n",
    "print(f'Train AUC: {train_auc}')\n",
    "\n",
    "# Probas for test\n",
    "y_test_lgbm_proba = lgbclf.predict_proba(X_test)[:, 1]  \n",
    "test_auc = roc_auc_score(y_test, y_test_lgbm_proba)\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Optimization using Random_Search as a sklearn built in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    " \"learning_rate\"    : [0.01, 0.05, 0.10, 0.15 ] ,\n",
    " \"max_depth\"        : [ 3, 6, 9, 12, 15],\n",
    " \"num_leaves\"       : [ 10, 500, 1000 ],\n",
    " \"n_estimators\"     : [ 0.1, 1, 10 , 100, 1000 ],\n",
    " \"subsample\"        : [ 0.1, 0.2, 0.3, 0.4, 0.5 , 0.7 , 0.8 , 0.9],\n",
    " \"reg_alpha\"        : [ 0.1, 0.3 , 0.6, 1 ],\n",
    " \"colsample_bytree\" : [ 0.1, 0.2, 0.3, 0.4, 0.5 , 0.7 , 0.8 , 0.9 ]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lgbclf = lgb.LGBMClassifier()\n",
    "\n",
    "random_search=RandomizedSearchCV(\n",
    "  lgbclf,\n",
    "  param_distributions=params,\n",
    "  n_iter=10,\n",
    "  scoring='roc_auc',\n",
    "  n_jobs=-1,\n",
    "  cv=5,\n",
    "  verbose=3)\n",
    "\n",
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Model With Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbclf_1 = lgb.LGBMClassifier(\n",
    "             \n",
    "        random_state=1003,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        metric='auc',\n",
    "        objective= 'binary',\n",
    "        device = 'gpu',\n",
    "        subsample= 0.3,\n",
    "        reg_alpha= 0.3,\n",
    "        num_leaves=500,\n",
    "        n_estimators= 1000,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.15,\n",
    "        colsample_bytree= 0.3\n",
    ")\n",
    "\n",
    "lgbclf_1.fit(X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_lgbm_1 = lgbclf_1.predict(X_test)\n",
    "\n",
    "# Probas for train\n",
    "y_train_lgbm_proba = lgbclf_1.predict_proba(X_train)[:, 1]  \n",
    "train_auc = roc_auc_score(y_train, y_train_lgbm_proba)\n",
    "print(f'Train AUC: {train_auc}')\n",
    "\n",
    "# Probas for test\n",
    "y_test_lgbm_proba = lgbclf_1.predict_proba(X_test)[:, 1]  \n",
    "test_auc = roc_auc_score(y_test, y_test_lgbm_proba)\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Feature Importance to Extract the most important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "cols = list( X_train.columns)\n",
    "feature_imp = pd.DataFrame(\n",
    "  sorted(zip(lgbclf_1.feature_importances_, cols), \n",
    "         key=lambda x: x[0], \n",
    "         reverse=True), \n",
    "  columns=['Value', 'Feature'])\n",
    "feature_imp.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 50 important features from X_train\n",
    "selected_features = feature_imp.head(50)['Feature'].tolist()\n",
    "\n",
    "# Creating new x dataframes\n",
    "X_train_lgbm_2 = X_train[selected_features ] \n",
    "X_test_lgbm_2 = X_test[selected_features ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted Features which using for thr transformation for the Deployments\n",
    "X_train_lgbm_2 = X_train[['card1',\n",
    " 'TransactionAmt',\n",
    " 'card2_target_encoded',\n",
    " 'addr_target_encoded',\n",
    " 'addr1_target_encoded',\n",
    " 'dist1',\n",
    " 'D15',\n",
    " 'id_02',\n",
    " 'TransactionAmt_decimal',\n",
    " 'C1',\n",
    " 'D4',\n",
    " 'card5_target_encoded',\n",
    " 'D2',\n",
    " 'D10',\n",
    " 'D11',\n",
    " 'V307',\n",
    " 'D1',\n",
    " 'D8',\n",
    " 'D5',\n",
    " 'V310',\n",
    " 'D3',\n",
    " 'id_05',\n",
    " 'V127',\n",
    " 'id_06',\n",
    " 'V314',\n",
    " 'D9',\n",
    " 'V264',\n",
    " 'V312',\n",
    " 'id_01',\n",
    " 'D14',\n",
    " 'V203',\n",
    " 'C5',\n",
    " 'D6',\n",
    " 'V283',\n",
    " 'D12',\n",
    " 'V36',\n",
    " 'V96',\n",
    " 'V221',\n",
    " 'V62',\n",
    " 'V82',\n",
    " 'V282',\n",
    " 'V54',\n",
    " 'V76',\n",
    " 'V37',\n",
    " 'V20',\n",
    " 'V5',\n",
    " 'V44',\n",
    " 'V285',\n",
    " 'V77',\n",
    " 'V56']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lgbm_2 = X_test[['card1',\n",
    " 'TransactionAmt',\n",
    " 'card2_target_encoded',\n",
    " 'addr_target_encoded',\n",
    " 'addr1_target_encoded',\n",
    " 'dist1',\n",
    " 'D15',\n",
    " 'id_02',\n",
    " 'TransactionAmt_decimal',\n",
    " 'C1',\n",
    " 'D4',\n",
    " 'card5_target_encoded',\n",
    " 'D2',\n",
    " 'D10',\n",
    " 'D11',\n",
    " 'V307',\n",
    " 'D1',\n",
    " 'D8',\n",
    " 'D5',\n",
    " 'V310',\n",
    " 'D3',\n",
    " 'id_05',\n",
    " 'V127',\n",
    " 'id_06',\n",
    " 'V314',\n",
    " 'D9',\n",
    " 'V264',\n",
    " 'V312',\n",
    " 'id_01',\n",
    " 'D14',\n",
    " 'V203',\n",
    " 'C5',\n",
    " 'D6',\n",
    " 'V283',\n",
    " 'D12',\n",
    " 'V36',\n",
    " 'V96',\n",
    " 'V221',\n",
    " 'V62',\n",
    " 'V82',\n",
    " 'V282',\n",
    " 'V54',\n",
    " 'V76',\n",
    " 'V37',\n",
    " 'V20',\n",
    " 'V5',\n",
    " 'V44',\n",
    " 'V285',\n",
    " 'V77',\n",
    " 'V56']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Run the model with top 50 important features (We did with 100 as well. but it results to more overfitted model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbclf_2 = lgb.LGBMClassifier(\n",
    "              \n",
    "        random_state=1003,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        metric='auc',\n",
    "        objective= 'binary',\n",
    "        device = 'gpu',\n",
    "        subsample= 0.3,\n",
    "        reg_alpha= 0.3,\n",
    "        num_leaves=500,\n",
    "        n_estimators= 1000,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.15,\n",
    "        colsample_bytree= 0.3\n",
    "    )\n",
    "\n",
    "lgbclf_2.fit(X_train_lgbm_2,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_lgbm_3 = lgbclf_2.predict(X_test_lgbm_2)\n",
    "\n",
    "# Probas for train\n",
    "y_train_lgbm_proba = lgbclf_2.predict_proba(X_train_lgbm_2)[:, 1]  \n",
    "train_auc = roc_auc_score(y_train, y_train_lgbm_proba)\n",
    "print(f'Train AUC: {train_auc}')\n",
    "\n",
    "# Probas for test\n",
    "y_test_lgbm_proba = lgbclf_2.predict_proba(X_test_lgbm_2)[:, 1]  \n",
    "test_auc = roc_auc_score(y_test, y_test_lgbm_proba)\n",
    "print(f'Test AUC: {test_auc}')"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning the Hyperparameters to reduce the Overfitting Issue :\n",
    "\n",
    "      To Reduce the Model Complexity we shoulde  Decrease \"num_leaves\" and \"max_depth\"\n",
    "\n",
    "      To Increase Regularization , we need to increase \"reg_alpha\"\n",
    "\n",
    "      To reduce the Overfitting, we may need to increase the \"subsample\" ratio to use a larger share of data for each boosting round.\n",
    "\n",
    "      Reducing the \"learning_rate\" slow down the convergence of the training process and can lead to more robust model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Info] Number of positive: 15563, number of negative: 427342\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7502\n",
      "[LightGBM] [Info] Number of data points in the train set: 442905, number of used features: 50\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 30 dense feature groups (13.52 MB) transferred to GPU in 0.020733 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035138 -> initscore=-3.312688\n",
      "[LightGBM] [Info] Start training from score -3.312688\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98    142535\n",
      "           1       0.49      0.40      0.44      5100\n",
      "\n",
      "    accuracy                           0.96    147635\n",
      "   macro avg       0.73      0.69      0.71    147635\n",
      "weighted avg       0.96      0.96      0.96    147635\n",
      "\n",
      "[[0.98521767 0.01478233]\n",
      " [0.60313725 0.39686275]]\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "Train AUC: 0.9174968929764022\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "Test AUC: 0.8773297236248131\n"
     ]
    }
   ],
   "source": [
    "lgbclf_3 = lgb.LGBMClassifier(\n",
    "              \n",
    "    random_state=1003,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        metric='auc',\n",
    "        objective= 'binary',\n",
    "        device = 'gpu',\n",
    "        subsample= 0.8,\n",
    "        reg_alpha= 0.5,\n",
    "        num_leaves=20,\n",
    "        n_estimators= 1000,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.01,        \n",
    "        colsample_bytree= 0.5,\n",
    "        init_score = 0.18,\n",
    "        boosting_type = 'gbdt'\n",
    "    )\n",
    "\n",
    "lgbclf_3.fit(X_train_lgbm_2,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_lgbm_3 = lgbclf_3.predict(X_test_lgbm_2)\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test, y_pred_lgbm_3))\n",
    "\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred_lgbm_3, normalize='true'))\n",
    "\n",
    "# Probas for train\n",
    "y_train_lgbm_proba = lgbclf_3.predict_proba(X_train_lgbm_2)[:, 1]  \n",
    "train_auc = roc_auc_score(y_train, y_train_lgbm_proba)\n",
    "print(f'Train AUC: {train_auc}')\n",
    "\n",
    "# Probas for test\n",
    "y_test_lgbm_proba = lgbclf_3.predict_proba(X_test_lgbm_2)[:, 1]  \n",
    "test_auc = roc_auc_score(y_test, y_test_lgbm_proba)\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=4, random_state=None, shuffle=True)\n",
      "0\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Info] Number of positive: 11645, number of negative: 320533\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 9874\n",
      "[LightGBM] [Info] Number of data points in the train set: 332178, number of used features: 216\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 46 dense feature groups (15.21 MB) transferred to GPU in 0.022416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035057 -> initscore=-3.315108\n",
      "[LightGBM] [Info] Start training from score -3.315108\n",
      "finish train\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "finish pred\n",
      "ROC accuracy: 0.9204336311743208\n",
      "1\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Info] Number of positive: 11660, number of negative: 320519\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 9881\n",
      "[LightGBM] [Info] Number of data points in the train set: 332179, number of used features: 216\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 46 dense feature groups (15.21 MB) transferred to GPU in 0.030686 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035102 -> initscore=-3.313777\n",
      "[LightGBM] [Info] Start training from score -3.313777\n",
      "finish train\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "finish pred\n",
      "ROC accuracy: 0.9144429615502351\n",
      "2\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Info] Number of positive: 11688, number of negative: 320491\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 9882\n",
      "[LightGBM] [Info] Number of data points in the train set: 332179, number of used features: 216\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 46 dense feature groups (15.21 MB) transferred to GPU in 0.030236 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035186 -> initscore=-3.311292\n",
      "[LightGBM] [Info] Start training from score -3.311292\n",
      "finish train\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "finish pred\n",
      "ROC accuracy: 0.9207500622663878\n",
      "3\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Info] Number of positive: 11696, number of negative: 320483\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 9869\n",
      "[LightGBM] [Info] Number of data points in the train set: 332179, number of used features: 216\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 46 dense feature groups (15.21 MB) transferred to GPU in 0.028834 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035210 -> initscore=-3.310582\n",
      "[LightGBM] [Info] Start training from score -3.310582\n",
      "finish train\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "[LightGBM] [Warning] Unknown parameter: init_score\n",
      "finish pred\n",
      "ROC accuracy: 0.9170323819695815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_fold = 4\n",
    "folds = KFold(n_splits=n_fold,shuffle=True)\n",
    "\n",
    "print(folds)\n",
    "\n",
    "\n",
    "lgb_CV=train.copy()\n",
    "lgb_CV['isFraud'] = 0\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "    print(fold_n)\n",
    "    \n",
    "    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    lgbclf = lgb.LGBMClassifier(\n",
    "        random_state=1003,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        metric='auc',\n",
    "        objective= 'binary',\n",
    "        device = 'gpu',\n",
    "        subsample= 0.8,\n",
    "        reg_alpha= 0.5,\n",
    "        num_leaves=20,\n",
    "        n_estimators= 1000,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.01,        \n",
    "        colsample_bytree= 0.5,\n",
    "        init_score = 0.18,\n",
    "        boosting_type = 'gbdt'\n",
    "    )\n",
    "    \n",
    "    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    lgbclf.fit(X_train_,y_train_)\n",
    "    \n",
    "    del X_train_,y_train_\n",
    "    print('finish train')\n",
    "    pred=lgbclf.predict_proba(X_test)[:,1]\n",
    "    val=lgbclf.predict_proba(X_valid)[:,1]\n",
    "    print('finish pred')\n",
    "    del lgbclf, X_valid\n",
    "    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n",
    "    del val,y_valid\n",
    "    del pred"
   ]
=======
>>>>>>> parent of bd47633 (Tunning the LightGBM Hyperparameters)
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
